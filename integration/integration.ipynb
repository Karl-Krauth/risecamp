{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RISE Camp Capstone Exercise\n",
    "\n",
    "**GOAL:** In this exercise, you will see how many of the projects you've learned about in the last couple days fit together. Those of you who attended last year's RISE Camp will remember the Pong integration exercise that trained an RL policy in Ray and deployed it in Clipper. Today, we're going to extend that verison.\n",
    "\n",
    "We will train _three_ RL policies in RLLib and Ray. Flor will track all the training process for all three agents. We will use Flor to compare the training processes for all three and show how a simple change in RLLib can significantly affect performance. Each one of the trained models will be granted special access with an authentication key using Wave. Each one of those models will then be deployed into Clipper, which will require the Wave authentication key in order to create the model container. \n",
    "\n",
    "Finally, you'll play a game (or more! against each of the three policies. We'll aggregate the results to see which agent performs the best."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
