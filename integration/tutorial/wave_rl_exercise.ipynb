{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exercise 6 - Training with Ray and Serving with Clipper\n",
    "\n",
    "**GOAL:** The goal of this exercise is to show how to train a policy with Ray and to deploy it with Clipper in a fun, interactive way.\n",
    "\n",
    "We will train an agent to play Pong, and then we will play Pong against the policy that we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING: lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import pong_py\n",
    "import ray\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.agents import ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process STDOUT and STDERR is being redirected to /tmp/raylogs/.\n",
      "Waiting for redis server at 127.0.0.1:52284 to respond...\n",
      "Waiting for redis server at 127.0.0.1:41463 to respond...\n",
      "Starting local scheduler with the following resources: {'CPU': 16, 'GPU': 1}.\n",
      "\n",
      "======================================================================\n",
      "View the web UI at http://localhost:8889/notebooks/ray_ui62916.ipynb?token=6142f365f2bf1eaef4bba03888053967b12167d2d9b2b039\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:52284',\n",
       " 'object_store_addresses': [ObjectStoreAddress(name='/tmp/plasma_store71471754', manager_name='/tmp/plasma_manager62828724', manager_port=53327)],\n",
       " 'local_scheduler_socket_names': ['/tmp/scheduler21957333'],\n",
       " 'raylet_socket_names': [],\n",
       " 'webui_url': 'http://localhost:8889/notebooks/ray_ui62916.ipynb?token=6142f365f2bf1eaef4bba03888053967b12167d2d9b2b039'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an agent that can be trained using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LogSyncer for /home/jovyan/ray_results/2018-09-21_20-22-229s806msu -> None\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "LocalMultiGPUOptimizer devices ['/cpu:0']\n"
     ]
    }
   ],
   "source": [
    "def env_creator(env_config):\n",
    "    return pong_py.PongJSEnv()\n",
    "\n",
    "register_env(\"my_env\", env_creator)\n",
    "trainer = ppo.PPOAgent(env=\"my_env\", config={\n",
    "    \"env_config\": {},  # config to pass to env creator\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the `PPOAgent` for some number of iterations.\n",
    "\n",
    "**EXERCISE:** You will need to experiment with the number of iterations as well as with the configuration to get the agent to learn something reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== sgd epochs ==\n",
      "0 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2298.363, 'policy_loss': -0.0010465819, 'vf_loss': 2298.364, 'vf_explained_var': 0.0017972627, 'kl': 4.816657e-05, 'entropy': 1.0985396}\n",
      "1 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2296.1841, 'policy_loss': -0.0020927656, 'vf_loss': 2296.186, 'vf_explained_var': 0.0056349211, 'kl': 0.00021130528, 'entropy': 1.0983686}\n",
      "2 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2294.9033, 'policy_loss': -0.0028287813, 'vf_loss': 2294.9062, 'vf_explained_var': 0.0091754002, 'kl': 0.00073410041, 'entropy': 1.0978278}\n",
      "3 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2294.3191, 'policy_loss': -0.0038047591, 'vf_loss': 2294.3228, 'vf_explained_var': 0.0089636752, 'kl': 0.0014569559, 'entropy': 1.0970924}\n",
      "4 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2293.7612, 'policy_loss': -0.004650441, 'vf_loss': 2293.7656, 'vf_explained_var': 0.0090345703, 'kl': 0.0026331909, 'entropy': 1.0959095}\n",
      "5 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2293.2192, 'policy_loss': -0.0056273774, 'vf_loss': 2293.2239, 'vf_explained_var': 0.0087462794, 'kl': 0.0038833576, 'entropy': 1.0946555}\n",
      "6 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2292.5906, 'policy_loss': -0.0064327628, 'vf_loss': 2292.5957, 'vf_explained_var': 0.0088242181, 'kl': 0.005678541, 'entropy': 1.0928473}\n",
      "7 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2291.8284, 'policy_loss': -0.0070796558, 'vf_loss': 2291.8342, 'vf_explained_var': 0.0084709255, 'kl': 0.0068436051, 'entropy': 1.0916945}\n",
      "8 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2290.8611, 'policy_loss': -0.0072649186, 'vf_loss': 2290.8667, 'vf_explained_var': 0.0079872413, 'kl': 0.0072973454, 'entropy': 1.0912476}\n",
      "9 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2289.6172, 'policy_loss': -0.0075654308, 'vf_loss': 2289.6235, 'vf_explained_var': 0.0073241671, 'kl': 0.0075257826, 'entropy': 1.0910217}\n",
      "10 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2288.0771, 'policy_loss': -0.007892753, 'vf_loss': 2288.083, 'vf_explained_var': 0.006891116, 'kl': 0.0089095067, 'entropy': 1.0896631}\n",
      "11 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2286.2, 'policy_loss': -0.0079190852, 'vf_loss': 2286.2061, 'vf_explained_var': 0.0060404097, 'kl': 0.0081804553, 'entropy': 1.0903811}\n",
      "12 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2284.0088, 'policy_loss': -0.0079610776, 'vf_loss': 2284.0149, 'vf_explained_var': 0.0048786653, 'kl': 0.0083276825, 'entropy': 1.0902133}\n",
      "13 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2281.5261, 'policy_loss': -0.0083298516, 'vf_loss': 2281.5325, 'vf_explained_var': 0.0036977511, 'kl': 0.008912704, 'entropy': 1.0896494}\n",
      "14 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2279.0779, 'policy_loss': -0.0084775807, 'vf_loss': 2279.0847, 'vf_explained_var': 0.0027148358, 'kl': 0.0089974804, 'entropy': 1.0895704}\n",
      "15 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2277.4221, 'policy_loss': -0.0086742202, 'vf_loss': 2277.4292, 'vf_explained_var': 0.0023183746, 'kl': 0.009202322, 'entropy': 1.0893776}\n",
      "16 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2276.6738, 'policy_loss': -0.0087280096, 'vf_loss': 2276.6812, 'vf_explained_var': 0.0020728977, 'kl': 0.0091658887, 'entropy': 1.089404}\n",
      "17 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2276.3206, 'policy_loss': -0.0090304818, 'vf_loss': 2276.3274, 'vf_explained_var': 0.0022740536, 'kl': 0.0098237339, 'entropy': 1.0887573}\n",
      "18 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2276.1565, 'policy_loss': -0.0092806993, 'vf_loss': 2276.1638, 'vf_explained_var': 0.002495283, 'kl': 0.010037305, 'entropy': 1.0885427}\n",
      "19 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2276.0647, 'policy_loss': -0.0094272299, 'vf_loss': 2276.0718, 'vf_explained_var': 0.0023424895, 'kl': 0.010193749, 'entropy': 1.0883979}\n",
      "20 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.9958, 'policy_loss': -0.0095107984, 'vf_loss': 2276.0037, 'vf_explained_var': 0.0023669004, 'kl': 0.0099819992, 'entropy': 1.0886066}\n",
      "21 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.9412, 'policy_loss': -0.0095777223, 'vf_loss': 2275.9482, 'vf_explained_var': 0.0027000597, 'kl': 0.010320459, 'entropy': 1.0882829}\n",
      "22 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.8918, 'policy_loss': -0.0099255694, 'vf_loss': 2275.8997, 'vf_explained_var': 0.0022798316, 'kl': 0.010914848, 'entropy': 1.0876834}\n",
      "23 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.8572, 'policy_loss': -0.010085475, 'vf_loss': 2275.865, 'vf_explained_var': 0.0027063105, 'kl': 0.010858152, 'entropy': 1.0877531}\n",
      "24 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.8276, 'policy_loss': -0.010264293, 'vf_loss': 2275.8357, 'vf_explained_var': 0.0025716771, 'kl': 0.010987697, 'entropy': 1.0876378}\n",
      "25 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.7925, 'policy_loss': -0.010312863, 'vf_loss': 2275.8005, 'vf_explained_var': 0.0025272658, 'kl': 0.010661163, 'entropy': 1.0879595}\n",
      "26 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.7698, 'policy_loss': -0.01053973, 'vf_loss': 2275.7781, 'vf_explained_var': 0.002603102, 'kl': 0.010914709, 'entropy': 1.0877007}\n",
      "27 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.7434, 'policy_loss': -0.010668478, 'vf_loss': 2275.752, 'vf_explained_var': 0.0025248067, 'kl': 0.011151079, 'entropy': 1.0874848}\n",
      "28 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.7202, 'policy_loss': -0.010712028, 'vf_loss': 2275.7285, 'vf_explained_var': 0.0024145553, 'kl': 0.011142942, 'entropy': 1.0874715}\n",
      "29 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2275.6995, 'policy_loss': -0.010919865, 'vf_loss': 2275.7083, 'vf_explained_var': 0.0023894482, 'kl': 0.011351176, 'entropy': 1.0872625}\n",
      "== sgd epochs ==\n",
      "0 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2107.8721, 'policy_loss': -0.0024960302, 'vf_loss': 2107.8745, 'vf_explained_var': 0.0096387938, 'kl': 3.6180223e-05, 'entropy': 1.088531}\n",
      "1 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2097.2725, 'policy_loss': -0.0032995283, 'vf_loss': 2097.2759, 'vf_explained_var': 0.0062346738, 'kl': 0.00027750048, 'entropy': 1.0897727}\n",
      "2 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2094.1631, 'policy_loss': -0.0042221025, 'vf_loss': 2094.167, 'vf_explained_var': 0.0047218632, 'kl': 0.0010613964, 'entropy': 1.0912195}\n",
      "3 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2094.0098, 'policy_loss': -0.0051983166, 'vf_loss': 2094.0146, 'vf_explained_var': 0.005016923, 'kl': 0.0019868384, 'entropy': 1.0910947}\n",
      "4 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9714, 'policy_loss': -0.0060228202, 'vf_loss': 2093.9768, 'vf_explained_var': 0.0047135763, 'kl': 0.0029943669, 'entropy': 1.0908325}\n",
      "5 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9546, 'policy_loss': -0.0068289675, 'vf_loss': 2093.9604, 'vf_explained_var': 0.0049701501, 'kl': 0.0042725978, 'entropy': 1.090976}\n",
      "6 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9377, 'policy_loss': -0.0074664345, 'vf_loss': 2093.9441, 'vf_explained_var': 0.0046103485, 'kl': 0.0054792827, 'entropy': 1.0906203}\n",
      "7 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9309, 'policy_loss': -0.0081964768, 'vf_loss': 2093.9375, 'vf_explained_var': 0.0046420544, 'kl': 0.0070686284, 'entropy': 1.0901639}\n",
      "8 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.925, 'policy_loss': -0.0087460978, 'vf_loss': 2093.9324, 'vf_explained_var': 0.0044373963, 'kl': 0.0080844201, 'entropy': 1.0886548}\n",
      "9 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9226, 'policy_loss': -0.0089950999, 'vf_loss': 2093.9302, 'vf_explained_var': 0.0043135621, 'kl': 0.0081307301, 'entropy': 1.0874684}\n",
      "10 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9211, 'policy_loss': -0.0092577133, 'vf_loss': 2093.9287, 'vf_explained_var': 0.0045384001, 'kl': 0.0088205738, 'entropy': 1.0866177}\n",
      "11 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9199, 'policy_loss': -0.00937723, 'vf_loss': 2093.9277, 'vf_explained_var': 0.0042967629, 'kl': 0.0089113023, 'entropy': 1.0866349}\n",
      "12 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9182, 'policy_loss': -0.0095668137, 'vf_loss': 2093.9258, 'vf_explained_var': 0.0043503121, 'kl': 0.0092629418, 'entropy': 1.086328}\n",
      "13 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.917, 'policy_loss': -0.0096977018, 'vf_loss': 2093.9248, 'vf_explained_var': 0.0043041781, 'kl': 0.0094743287, 'entropy': 1.0863757}\n",
      "14 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9163, 'policy_loss': -0.0097071845, 'vf_loss': 2093.9241, 'vf_explained_var': 0.0041967444, 'kl': 0.0095492648, 'entropy': 1.0864878}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.916, 'policy_loss': -0.0096406806, 'vf_loss': 2093.9238, 'vf_explained_var': 0.00436287, 'kl': 0.0092345923, 'entropy': 1.0863502}\n",
      "16 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.915, 'policy_loss': -0.0099215694, 'vf_loss': 2093.9231, 'vf_explained_var': 0.0041438546, 'kl': 0.0096677803, 'entropy': 1.0863003}\n",
      "17 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.915, 'policy_loss': -0.0099207554, 'vf_loss': 2093.9231, 'vf_explained_var': 0.0041349549, 'kl': 0.0087499479, 'entropy': 1.08655}\n",
      "18 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.915, 'policy_loss': -0.0098973876, 'vf_loss': 2093.9229, 'vf_explained_var': 0.0041603167, 'kl': 0.0096352277, 'entropy': 1.0856956}\n",
      "19 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9146, 'policy_loss': -0.010132942, 'vf_loss': 2093.9229, 'vf_explained_var': 0.0040607341, 'kl': 0.0093232822, 'entropy': 1.0865316}\n",
      "20 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9141, 'policy_loss': -0.010284822, 'vf_loss': 2093.9226, 'vf_explained_var': 0.004114043, 'kl': 0.0096248779, 'entropy': 1.0859239}\n",
      "21 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9141, 'policy_loss': -0.010332022, 'vf_loss': 2093.9224, 'vf_explained_var': 0.0041386168, 'kl': 0.0098418696, 'entropy': 1.0863943}\n",
      "22 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9141, 'policy_loss': -0.010187075, 'vf_loss': 2093.9224, 'vf_explained_var': 0.0041010622, 'kl': 0.0091252681, 'entropy': 1.0858595}\n",
      "23 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9136, 'policy_loss': -0.010453822, 'vf_loss': 2093.9224, 'vf_explained_var': 0.0040515643, 'kl': 0.009479966, 'entropy': 1.0858667}\n",
      "24 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9136, 'policy_loss': -0.010532238, 'vf_loss': 2093.9221, 'vf_explained_var': 0.0040458385, 'kl': 0.0098775234, 'entropy': 1.0856752}\n",
      "25 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9133, 'policy_loss': -0.01070034, 'vf_loss': 2093.9224, 'vf_explained_var': 0.0040391497, 'kl': 0.0096514132, 'entropy': 1.0853781}\n",
      "26 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9136, 'policy_loss': -0.010626996, 'vf_loss': 2093.9221, 'vf_explained_var': 0.0040365327, 'kl': 0.0093590617, 'entropy': 1.0864985}\n",
      "27 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9131, 'policy_loss': -0.010907974, 'vf_loss': 2093.9221, 'vf_explained_var': 0.0039605852, 'kl': 0.0098863952, 'entropy': 1.0858197}\n",
      "28 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9133, 'policy_loss': -0.01068239, 'vf_loss': 2093.9224, 'vf_explained_var': 0.0040046722, 'kl': 0.0092964508, 'entropy': 1.0853341}\n",
      "29 {'cur_lr': 4.9999998736893758e-05, 'total_loss': 2093.9131, 'policy_loss': -0.010688316, 'vf_loss': 2093.9221, 'vf_explained_var': 0.0040615182, 'kl': 0.0092582311, 'entropy': 1.0855513}\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the agent manually by calling `agent.compute_action` and see the rewards you get are consistent with the rewards printed during the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "40\n",
      "40\n",
      "163\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "264\n",
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "env = pong_py.PongJSEnv()\n",
    "\n",
    "for _ in range(20):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = trainer.compute_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "\n",
    "    print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint the agent so that the relevant model can be saved and deployed to Clipper. We save the name of the checkpoint file in `metadata.json` so the model container knows how to restore the policy checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "checkpoint_path = trainer.save()\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_file = os.path.basename(checkpoint_path)\n",
    "with open(os.path.join(checkpoint_dir, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump({\"checkpoint\": checkpoint_file}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "\n",
    "model = io.BytesIO()\n",
    "with tarfile.open(fileobj=model, mode=\"w:gz\") as tar:\n",
    "        tar.add(checkpoint_dir, arcname=os.path.basename(checkpoint_dir))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypt_response = wave.EncryptMessage(\n",
    "    wv.EncryptMessageParams(\n",
    "        namespace=namespace,\n",
    "        resource=\"models/pong\",\n",
    "        content=bytes(model.getvalue(),\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciphertext = encrypt_response.ciphertext\n",
    "\n",
    "decrypt_response = wave.DecryptMessage(wv.DecryptMessageParams(\n",
    "        perspective= perspective,\n",
    "        ciphertext= ciphertext,\n",
    "        resyncFirst= True))\n",
    "if resp.error.code != 0:\n",
    "    raise Exception(resp.error.Message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decryptedpath = \"outputmodel\"\n",
    "decrypted_file = os.path.join(decryptedpath,checkpoint_file)\n",
    "decryptedmodel = io.BytesIO(plaintext)\n",
    "with tarfile.open(fileobj=decryptedmodel, mode=\"r:gz\") as tar:\n",
    "    tar.extractall(path=decryptedpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Against the Policy\n",
    "\n",
    "In this section, we will play Pong against the policy that we just trained. The game will be played in your browser, and the policy that we trained will be served by Clipper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Deploy your policy using Clipper. Follow the instructions that get printed below to play Pong against the deployed policy. You'll need to deploy all of the data that is saved in the directory `os.path.dirname(checkpoint_path)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the `clipper_admin` library and use that to create a new Clipper instance to serve the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create your ClipperConnection, you need to tell it how to communicate with the Docker service and Clipper. You can use the following command to get the Docker IP address. Use that address when you create your `ClipperConnection` in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make logging work correctly in the Jupyter notebook\n",
    "import logging\n",
    "import sys\n",
    "import subprocess32 as subprocess\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from clipper_admin import DockerContainerManager, ClipperConnection\n",
    "docker_ip = subprocess.check_output(\"./get_docker_ip.sh\").strip()\n",
    "clipper_conn = ClipperConnection(DockerContainerManager(docker_ip_address=docker_ip))\n",
    "# Add a call to stop all in case you still have Clipper running from the earlier exercises\n",
    "clipper_conn.stop_all()\n",
    "clipper_conn.start_clipper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, deploy the saved policy checkpoint to Clipper using a Docker image we created for this exercise (similar to the TensorFlow model container in the Clipper tutorial). If you're curious, you can find the custom model container code on [GitHub](https://github.com/ucbrise/risecamp/blob/077aa51078e2043d4d3d2d539e256c30c259678e/rl_and_pong/pong_model_container.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = \"pong-policy\"\n",
    "app_name = \"pong\"\n",
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=1,\n",
    "    input_type=\"doubles\",\n",
    "    model_data_path=os.path.dirname(checkpoint_path),\n",
    "    base_image=\"clipper/risecamp-pong-container\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register a Clipper application and link it the deployed policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_name = \"pong\"\n",
    "clipper_conn.register_application(name=app_name, default_output=\"0\", input_type=\"doubles\", slo_micros=100000)\n",
    "clipper_conn.link_model_to_app(app_name=app_name, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have deployed your policy to Clipper, you will start a Pong application that will let you play against your policy in the browser.\n",
    "\n",
    "When you start the application, you need to tell it where Clipper is running in order for the Pong application to request predictions from Clipper. `ClipperConnection` provides the `get_query_addr()` method to get the IP address and port on which Clipper is listening for incoming prediction requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clipper_addr = clipper_conn.get_query_addr()\n",
    "print(\"Clipper address: {}\".format(clipper_addr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start the Pong webserver. It will print out the URL it's running on after it starts. Copy and paste that URL into your browser and press \"1\" to play against your policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess32 as subprocess\n",
    "server_handle = subprocess.Popen([\"./start_webserver.sh\", clipper_addr], stdout=subprocess.PIPE)\n",
    "print(server_handle.stdout.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a New Policy\n",
    "\n",
    "The first policy that you deploy probably won't be a very strong competitor, especially if you only trained it for a few iterations. Try training it for more iterations and deploying the new policy to Clipper. Clipper will automatically switch the Pong application to query the new version of the policy. You don't need to reload the page or even restart the game.\n",
    "\n",
    "For your convenience, we've copied the relevant cells from above to train the policy for more iterations and deploy it Clipper. You can run this cell as many times as you want. Don't forget to increment the version number of the model each time you deploy to Clipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train for more iterations\n",
    "for i in range(50):\n",
    "    result = agent.train()\n",
    "    \n",
    "# Save the new policy\n",
    "checkpoint_path = agent.save()\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_file = os.path.basename(checkpoint_path)\n",
    "with open(os.path.join(checkpoint_dir, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump({\"checkpoint\": checkpoint_file}, f)\n",
    "    \n",
    "# Deploy the new policy to Clipper.\n",
    "clipper_conn.build_and_deploy_model(\n",
    "    name=model_name,\n",
    "    version=2, # If you run this more than once, don't forget to keep updating the version.\n",
    "    input_type=\"doubles\",\n",
    "    model_data_path=os.path.dirname(checkpoint_path),\n",
    "    base_image=\"clipper/risecamp-pong-container\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
